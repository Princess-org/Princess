import map
import set
import util

export type Location = struct {
    line: int
    column: int
    end_line: int
    end_column: int
    file: Str
}

export type TokenKind = enum {
    UNKNOWN; EOF; EOL; START

    K_AUTO; K_BREAK; K_CASE; K_CHAR; K_CONST; K_CONTINUE
    K_DEFAULT; K_DO; K_DOUBLE; K_ELSE; K_ENUM; K_EXTERN
    K_FLOAT; K_FOR; K_GOTO; K_IF; K_INLINE; K_INT; K_LONG
    K_REGISTER; K_RESTRICT; K_RETURN; K_SHORT; K_SIGNED
    K_SIZEOF; K_STATIC; K_STRUCT; K_SWITCH; K_TYPEDEF; K_UNION
    K_UNSIGNED; K_VOID; K_VOLATILE; K_WHILE; K_BOOL

    P_DEFINED; P_MACRO; P_MACRO_FUNLIKE; P_IF; P_ELSE; P_ELIF; P_ENDIF;
    P_IFDEF; P_IFNDEF; P_LINE; P_PRAGMA; P_UNDEF; P_INCLUDE

    STRINGIFY; CONCAT; INCLUDE_ANGLE; INCLUDE_QOUTE

    OP_ASSIGN; OP_ADD; OP_SUB; OP_MUL; OP_DIV
    OP_MOD; OP_BOR; OP_BAND; OP_BXOR; OP_BNOT; OP_SHL
    OP_SHR; OP_INC; OP_DEC; OP_EQ; OP_NEQ; OP_LEQ; OP_GEQ
    OP_LT; OP_GT; OP_ADD_EQ; OP_SUB_EQ; OP_MUL_EQ; OP_DIV_EQ
    OP_MOD_EQ; OP_OR_EQ; OP_AND_EQ; OP_XOR_EQ; OP_SHL_EQ
    OP_SHR_EQ; OP_DOT; OP_VARARGS; OP_NOT; OP_AND; OP_OR
    OP_COLON; OP_ARROW

    O_BRACE; C_BRACE
    O_SQUARE; C_SQUARE
    O_PAREN; C_PAREN

    QUESTION_MARK; COMMA; SEMICOLON

    IDENTIFIER
    INTEGER
    CHARACTER
    FLOAT
    STRING
}

let keywords = map::make(TokenKind)
keywords["auto"] = TokenKind::K_AUTO
keywords["break"] = TokenKind::K_BREAK
keywords["case"] = TokenKind::K_CASE
keywords["char"] = TokenKind::K_CHAR
keywords["const"] = TokenKind::K_CONST
keywords["continue"] = TokenKind::K_CONTINUE
keywords["default"] = TokenKind::K_DEFAULT
keywords["do"] = TokenKind::K_DO
keywords["double"] = TokenKind::K_DOUBLE
keywords["else"] = TokenKind::K_ELSE
keywords["enum"] = TokenKind::K_ENUM
keywords["extern"] = TokenKind::K_EXTERN
keywords["float"] = TokenKind::K_FLOAT
keywords["for"] = TokenKind::K_FOR
keywords["goto"] = TokenKind::K_GOTO
keywords["if"] = TokenKind::K_IF
keywords["inline"] = TokenKind::K_INLINE
keywords["int"] = TokenKind::K_INT
keywords["long"] = TokenKind::K_LONG
keywords["register"] = TokenKind::K_REGISTER
keywords["restrict"] = TokenKind::K_RESTRICT
keywords["return"] = TokenKind::K_RETURN
keywords["short"] = TokenKind::K_SHORT
keywords["signed"] = TokenKind::K_SIGNED
keywords["sizeof"] = TokenKind::K_SIZEOF
keywords["static"] = TokenKind::K_STATIC
keywords["struct"] = TokenKind::K_STRUCT
keywords["switch"] = TokenKind::K_SWITCH
keywords["typedef"] = TokenKind::K_TYPEDEF
keywords["union"] = TokenKind::K_UNION
keywords["unsigned"] = TokenKind::K_UNSIGNED
keywords["void"] = TokenKind::K_VOID
keywords["volatile"] = TokenKind::K_VOLATILE
keywords["while"] = TokenKind::K_WHILE
keywords["_Bool"] = TokenKind::K_BOOL

let prep_directives = map::make(TokenKind)
prep_directives["define"] = TokenKind::P_MACRO
prep_directives["if"] = TokenKind::P_IF
prep_directives["else"] = TokenKind::P_ELSE
prep_directives["elif"] = TokenKind::P_ELIF
prep_directives["endif"] = TokenKind::P_ENDIF
prep_directives["ifdef"] = TokenKind::P_IFDEF
prep_directives["ifndef"] = TokenKind::P_IFNDEF
prep_directives["undef"] = TokenKind::P_UNDEF
prep_directives["include"] = TokenKind::P_INCLUDE

export type Token = struct {
    kind: TokenKind
    loc: Location
    value: struct #union {
        str: StringSlice
        i: uint64
        f: double
    }
}

export def is_textual(kind: TokenKind) -> bool {
    return kind == TokenKind::STRING or
        kind == TokenKind::IDENTIFIER or
        kind >= TokenKind::K_AUTO and kind <= TokenKind::K_BOOL
}

export def destruct(token: *Token) {
    if is_textual(token.kind) {
        __destruct__(*token.value.str)
    }
}

export def construct(copy: *Token, this: *Token) {
    copy.kind = this.kind
    copy.loc = this.loc

    if is_textual(copy.kind) {
        copy.value.str = this.value.str
    } else {
        copy.value = this.value
    }
}

export type TokenList = struct {
    token: Token
    next: *TokenList
}

export def destruct(list: *TokenList) {
    list = list.next
    while list {
        let next = list.next
        __destruct__(*list.token)
        free(list)
        list = next
    }
}

type State = struct {
    i: int
    line: int
    column: int
    input: Str
    file: Str
}

const EOF: char = 0x1A !char

def peek(state: *State, ahead: int = 0) -> char {
    def _peek(ahead: int) -> char {
        if state.i + ahead >= state.input.length() {
            return EOF
        }
        return state.input[state.i + ahead]
    }

    var n = 0
    loop {
        var ch = _peek(n)
        var ch2 = ch
        while ch2 == '\\' {
            ch2 = _peek(1 + n)
            if ch2 == '\r' {
                ch2 = _peek(2 + n)
                if ch2 == '\n' {
                    state.line += 1
                    state.column = 0
                    ch2 = ch = _peek(3 + n)
                    state.i += 3
                }
            } else if ch2 == '\n' {
                state.line += 1
                state.column = 0
                ch2 = ch = _peek(2 + n)
                state.i += 2
            }
        }
        if n == ahead { return ch }
        n += 1
    }
}

def pop(state: *State) -> char {
    let ch = peek(state, 1)
    if ch == '\n' {
        state.line += 1
        state.column = 0
    } else if ch != EOF {
        state.column += 1
    }
    state.i += 1
    return ch
}

def is_text(a: char) -> bool {
    return a == '_' or a >= 'a' and a <= 'z' or a >= 'A' and a <= 'Z'
}

def is_alphanumeric(a: char) -> bool {
    return is_text(a) or a >= '0' and a <= '9'
}

def is_hex_digit(a: char) -> bool {
    return get_hex_digit(a) >= 0
}

def get_hex_digit(a: char) -> int {
    if a >= '0' and a <= '9' { return a - '0' }
    if a >= 'a' and a <= 'f' { return a - 'a' }
    if a >= 'A' and a <= 'F' { return a - 'A' }
    return -1
}

def lex_identifier(state: *State) -> Token {
    let start = state.i
    var end = state.i
    var next = peek(state)
    while next != EOF and is_alphanumeric(next) {
        pop(state)
        end += 1
        next = peek(state)
    }
    let res = state.input.slice(start, end)
    if keywords.contains(res) {
        let token = { kind = keywords[res] } !Token
        token.value.str = res
    }
    let token = { kind = TokenKind::IDENTIFIER } !Token
    token.value.str = res
    return token
}

def lex_float(state: *State) -> Token {
    var off: int = 0
    let res = util::parse_float(state.input.slice(state.i, length(state.input)), *off)
    state.i += off

    let tok = { kind = TokenKind::FLOAT } !Token
    tok.value.f = res
    return tok
}

def lex_int(state: *State) -> Token {
    let c = peek(state)
    var res: uint64
    var off: int = 0

    let substr = state.input.slice(state.i, length(state.input))
    if c == '0' {
        let r = peek(state, 1)
        if r == 'b' {
            off += 2
            res = util::parse_number(substr, "01", *off)
        } else if r == 'x' {
            off += 2
            res = util::parse_number(substr, "0123456789abcdef", *off)
        } else {
            res = util::parse_number(substr, "01234567", *off)
        }
    } else {
        res = util::parse_number(substr, "0123456789", *off)
    }
    state.i += off
    let tok = { kind = TokenKind::INTEGER } !Token
    tok.value.i = res
    return tok
}

def lex_number(state: *State) -> Token {
    var j = 0
    var is_float = false
    loop {
        let c = peek(state, j)
        if c == '.' {
            let c2 = peek(state, j + 1)
            if c2 != '.' {
                is_float = true
            }
            break
        } else if c == 'e' or c == 'E' {
            is_float = true
            break
        } else if c >= '0' and c <= '9' {
            j += 1
        } else {
            break
        }
    }

    if is_float {
        return lex_float(state)
    }
    return lex_int(state)
}

def lex_escape_sequence(state: *State) -> Str {
    var str: StringBuffer = ""
    var ch = pop(state)
    switch ch {
        case 'a': str += '\a';   pop(state)
        case 'b': str += '\b';   pop(state)
        case 'e': str += '\x1B'; pop(state)
        case 'f': str += '\f';   pop(state)
        case 'n': str += '\n';   pop(state)
        case 'r': str += '\r';   pop(state)
        case 't': str += '\t';   pop(state)
        case 'v': str += '\v';   pop(state)
        case '\\': str += '\\';  pop(state)
        case '\'': str += '\'';  pop(state)
        case '"': str += '"';    pop(state)
        case '?': str += '?';    pop(state)
        case '0'..'8':
            var i = 0
            var res: char = 0
            while ch != EOF and ch >= '0' and ch <= '8' and i < 3 {
                res *= 8
                res += ch - '0'
                ch = pop(state)
                i += 1
            }
            str += res
        case 'x', 'u', 'U':
            ch = pop(state)
            var i = 0
            let n = MAX_INT32 if ch == 'x' else 4 if ch == 'u' else 8
            var res: char = 0
            while ch != EOF and is_hex_digit(ch) and i < n {
                res *= 16
                res += get_hex_digit(ch)
                ch = pop(state)
                i += 1
            }
            str += res
    }
    return str
}

def lex_string(state: *State) -> Token {
    pop(state)
    var ch = peek(state)
    var str: StringBuffer = ""
    while ch != EOF and ch != '"' and ch != '\n' {
        if ch == '\\' {
            str += lex_escape_sequence(state)
            ch = peek(state)
        } else {
            str += ch
            ch = pop(state)
        }
    }
    pop(state)

    let tok = { kind = TokenKind::STRING } !Token
    tok.value.str = to_slice(to_str(str))
    return tok
}

def lex_char(state: *State) -> Token {
    pop(state)
    var ch = peek(state)
    if ch == '\\' {
        let str = lex_escape_sequence(state)
        if str {
            ch = str[0]
        }
    }
    pop(state)
    pop(state)

    let tok = { kind = TokenKind::CHARACTER } !Token
    tok.value.i = ch !uint64
    return tok
}

def lex_symbol(state: *State) -> Token {
    let first = peek(state, 0)
    let second = peek(state, 1)
    let third = peek(state, 2)

    var tt: TokenKind = TokenKind::UNKNOWN
    var length = 3

    if first == '<' and second == '<' and third == '=' {
        tt = TokenKind::OP_SHL_EQ
    } else if first == '>' and second == '>' and third == '=' {
        tt = TokenKind::OP_SHR_EQ
    } else if first == '.' and second == '.' and third == '.' {
        tt = TokenKind::OP_VARARGS
    } else {
        length = 2
        if second == '=' {
            switch first {
                case '+': tt = TokenKind::OP_ADD_EQ
                case '-': tt = TokenKind::OP_SUB_EQ
                case '*': tt = TokenKind::OP_MUL_EQ
                case '/': tt = TokenKind::OP_DIV_EQ
                case '%': tt = TokenKind::OP_MOD_EQ
                case '&': tt = TokenKind::OP_AND_EQ
                case '|': tt = TokenKind::OP_OR_EQ
                case '^': tt = TokenKind::OP_XOR_EQ
                case '!': tt = TokenKind::OP_NEQ
                case '>': tt = TokenKind::OP_GEQ
                case '<': tt = TokenKind::OP_LEQ
                case '=': tt = TokenKind::OP_EQ
            }
        } else if first == '-' and second == '>' {
            tt = TokenKind::OP_ARROW
        } else if first == '+' and second == '+' {
            tt = TokenKind::OP_INC
        } else if first == '-' and second == '-' {
            tt = TokenKind::OP_DEC
        } else if first == '<' and second == '<' {
            tt = TokenKind::OP_SHL
        } else if first == '>' and second == '>' {
            tt = TokenKind::OP_SHR
        } else if first == '&' and second == '&' {
            tt = TokenKind::OP_AND
        } else if first == '|' and second == '|' {
            tt = TokenKind::OP_OR
        } else {
            length = 1
            switch first {
                case '>': tt = TokenKind::OP_GT
                case '<': tt = TokenKind::OP_LT
                case '{': tt = TokenKind::O_BRACE
                case '}': tt = TokenKind::C_BRACE
                case '[': tt = TokenKind::O_SQUARE
                case ']': tt = TokenKind::C_SQUARE
                case '(': tt = TokenKind::O_PAREN
                case ')': tt = TokenKind::C_PAREN
                case '+': tt = TokenKind::OP_ADD
                case '-': tt = TokenKind::OP_SUB
                case '*': tt = TokenKind::OP_MUL
                case '/': tt = TokenKind::OP_DIV
                case '%': tt = TokenKind::OP_MOD
                case '&': tt = TokenKind::OP_BAND
                case '|': tt = TokenKind::OP_BOR
                case '^': tt = TokenKind::OP_BXOR
                case '~': tt = TokenKind::OP_BNOT
                case '!': tt = TokenKind::OP_NOT
                case '=': tt = TokenKind::OP_ASSIGN                    
                case ',': tt = TokenKind::COMMA  
                case ':': tt = TokenKind::OP_COLON
                case '?': tt = TokenKind::QUESTION_MARK
                case '.': tt = TokenKind::OP_DOT
                case ';': tt = TokenKind::SEMICOLON
            }
        }
    }

    for var j in 0..length {
        pop(state)
    }

    return { kind = tt } !Token
}

def lex_next_token(state: *State) -> Token {
    let start_line = state.line
    let start_col = state.column

    var token: Token
    loop {
        var ch = peek(state)
        switch ch {
            case EOF: 
                pop(state)
            case '\n', '\r', ' ', '\t': 
                pop(state)
                continue
            case '0'..='9': token = lex_number(state)
            case 'a'..='z', 'A'..'Z', '_': token = lex_identifier(state)
            case '"': token = lex_string(state)
            case '\'': token = lex_char(state)
            case '/':
                let second = peek(state, 1)
                if second == '*' {
                    pop(state)
                    pop(state)
                    while ch != '*' and peek(state, 1) != '/' and ch != EOF {
                        ch = pop(state)
                    }
                    pop(state)
                } else if second == '/' {
                    pop(state)
                    pop(state)
                    while ch != '\n' and ch != EOF {
                        ch = pop(state)
                    }
                } else {
                    token = lex_symbol(state)
                }
            case '#': break
            case: token = lex_symbol(state)
        }
        break
    }

    token.loc = {
        line = start_line,
        column = start_col,
        end_line = state.line,
        end_column = state.column,
        file = state.file
    } !Location

    return token
}

def lex_directive(state: *State) -> Token {
    var name: Token
    while state.i < state.input.length() {
        name = lex_next_token(state)
        if is_textual(name.kind) or name.kind == TokenKind::EOF {
            break
        }
    }
    
    var directive: TokenKind
    if prep_directives.contains(name.value.str) {
        directive = prep_directives[name.value.str]
    }

    if directive != TokenKind::P_MACRO {
        yield { kind = directive, loc = name.loc } !Token
    }

    var ch = peek(state)
    if directive == TokenKind::P_INCLUDE {
        while ch != TokenKind::EOF and ch == ' ' or ch == '\t' or ch == '\n' {
            ch = pop(state)
        }
        var start = state.i + 1

        var kind = TokenKind::INCLUDE_QOUTE
        if ch == '<' { kind = TokenKind::INCLUDE_ANGLE}
        if ch == '<' {
            ch = pop(state)
            while ch != '>' { ch = pop(state) }
        } else if ch == '"' {
            ch = pop(state)
            while ch != '"' { ch = pop(state) }
        }
        ch = pop(state)
        let token = { kind = kind } !Token
        token.value.str = state.input.slice(start, state.i - 1)

        yield token
    }

    var first_token = true
    while state.i < state.input.length() {
        let start_line = state.line
        let start_col = state.column

        var token: Token
        var ch = peek(state)
        switch ch {
            case '\n': 
                token = { kind = TokenKind::EOL } !Token
                pop(state)
            case '#':
                pop(state)
                token = { kind = TokenKind::STRINGIFY } !Token
                if peek(state) == '#' {
                    pop(state)
                    token = { kind = TokenKind::CONCAT } !Token
                }
            case: 
                let next = lex_next_token(state)
                if is_textual(next.kind) and directive == TokenKind::P_MACRO and first_token {
                    if peek(state) == '(' {
                        yield { kind = TokenKind::P_MACRO_FUNLIKE, loc = name.loc } !Token
                    } else {
                        yield { kind = directive, loc = name.loc } !Token
                    }
                    first_token = false
                }
                yield next
                continue
                
        }

        first_token = false

        token.loc = {
            line = start_line,
            column = start_col,
            end_line = state.line,
            end_column = state.column,
            file = state.file
        } !Location
        yield token

        if token.kind == TokenKind::EOL {
            return
        }
    }
}

export def lex(path: Str) -> *TokenList {
    let fp = open(path, "r")
    let input = read_all(fp)
    close(fp)

    return lex(input, path)
}

export def lex(input: Str, path: Str) -> *TokenList {
    let state = {
        input = input,
        file = path
    } !State
    
    var first = zero_allocate(TokenList)
    var res = first
    res.token.kind = TokenKind::START
    res.next = zero_allocate(TokenList)
    res = res.next
    for var token in lex(*state) {
        if token.kind == TokenKind::UNKNOWN {
            continue
        }
        let next = zero_allocate(TokenList)
        res.token = token
        res.next = next
        res = next
    }

    res.token.kind = TokenKind::EOF
    return first
}

def lex(state: *State) -> Token {
    while state.i < state.input.length() {
        var token: Token
        var ch = peek(state)
        if ch == '#' {
            pop(state)
            yield from lex_directive(state)
        } else {
            token = lex_next_token(state)
            yield token
        }
    }
}

export def print_tokens(list: *TokenList) {
    loop {
        let token = list.token
        print(to_string(token.kind))
        if token.kind == TokenKind::INTEGER {
            print("(", token.value.i, ")")
        } else if token.kind == TokenKind::CHARACTER {
            print("('", token.value.i !char, "')")
        } else if token.kind == TokenKind::FLOAT {
            print("(", token.value.f, ")")
        } else if token.kind == TokenKind::IDENTIFIER {
            print("(", token.value.str, ")")
        } else if token.kind == TokenKind::STRING or
            token.kind == TokenKind::INCLUDE_ANGLE or 
            token.kind == TokenKind::INCLUDE_QOUTE {

            print("(\"", token.value.str, "\")")
        }
        print(" ")
        if not list.next {
            return
        } else {
            list = list.next
        }
    }
    print("\n")
}